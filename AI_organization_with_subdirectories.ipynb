{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c2eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scanning directory: C:\\Users\\gabel\\OneDrive - Virginia Tech ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabel\\AppData\\Local\\Temp\\ipykernel_28640\\1478939144.py:47: DtypeWarning: Columns (8,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Nested Directory Structure (JSON Output) ---\n",
      "Output saved to: C:\\Users\\gabel\\OneDrive - Virginia Tech\\full_data_file_info.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import mimetypes\n",
    "import fitz # PyMuPDF\n",
    "import pandas as pd\n",
    "import docx\n",
    "from collections import defaultdict\n",
    "\n",
    "DOCUMENTS_DIR = Path(r\"C:\\Users\\gabel\\OneDrive - Virginia Tech\") #Set path to the directory you want to search\n",
    "EXPECTED_EXTENSIONS = {\n",
    "    \".pdf\", \".txt\", \".py\", \".md\", \".ipynb\", \".rmd\", \".r\", \".csv\",\n",
    "    \".xlsx\", \".xls\", \".docx\"\n",
    "}\n",
    "\n",
    "\n",
    "def is_hidden(path: Path) -> bool:\n",
    "    \"\"\"Return True if *any* part of the path (file or folder) starts with a dot.\"\"\"\n",
    "    return any(part.startswith(\".\") for part in path.parts)\n",
    "\n",
    "def list_files_and_dirs(root: Path):\n",
    "    \"\"\"\n",
    "    Recursively yield files and directories under `root`,\n",
    "    skipping hidden paths and unwanted extensions for files.\n",
    "    Returns lists of (full_path, is_dir).\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if is_hidden(p):\n",
    "            continue\n",
    "\n",
    "        if p.is_file():\n",
    "            if p.suffix.lower() in EXPECTED_EXTENSIONS:\n",
    "                all_items.append((p, False)) # (full_path, is_file)\n",
    "        elif p.is_dir():\n",
    "            all_items.append((p, True)) # (full_path, is_dir)\n",
    "    return all_items\n",
    "\n",
    "def getting_info_excel_csv_files(full_path: Path) -> str:\n",
    "    \"\"\"Return a textual preview/placeholder for Excel and CSV files.\"\"\"\n",
    "    try:\n",
    "        if full_path.suffix.lower() in {\".xlsx\", \".xls\"}:\n",
    "            df = pd.read_excel(full_path)\n",
    "        elif full_path.suffix.lower() == \".csv\":\n",
    "            df = pd.read_csv(full_path)\n",
    "        else:\n",
    "            return \"[Unknown file type]\"\n",
    "\n",
    "        preview = df.head().to_string(index=False)\n",
    "        col_names = df.columns.tolist()\n",
    "        row_names = df.index.tolist()\n",
    "        preview = f\"Columns: {', '.join(col_names)}\\nRows: {', '.join(map(str, row_names))}\\n{preview}\"\n",
    "        return preview[:2000]\n",
    "\n",
    "    except Exception as exc:\n",
    "        return f\"[Error reading file: {exc}]\"\n",
    "\n",
    "def get_info_from_docx(full_path: Path) -> str:\n",
    "    \"\"\"Return a textual preview/placeholder for Word documents.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(full_path)\n",
    "        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
    "        return \"\\n\".join(paragraphs)[:2000]\n",
    "    except Exception as exc:\n",
    "        return f\"[Error reading file: {exc}]\"\n",
    "\n",
    "def get_file_preview(path: Path, max_chars: int = 2000) -> str:\n",
    "    \"\"\"Return a textual preview/placeholder for arbitrary file types.\"\"\"\n",
    "    try:\n",
    "        if path.suffix.lower() == \".ipynb\":\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                nb = json.load(f)\n",
    "            cells = [\n",
    "                (\"[Markdown]\" if c[\"cell_type\"] == \"markdown\" else \"[Code]\") +\n",
    "                \"\\n\" + \"\".join(c[\"source\"]).strip()\n",
    "                for c in nb.get(\"cells\", [])\n",
    "                if c[\"cell_type\"] in {\"markdown\", \"code\"}\n",
    "            ]\n",
    "            return \"\\n\\n\".join(cells)[:max_chars]\n",
    "\n",
    "        elif path.suffix.lower() in {\".xlsx\", \".xls\", \".csv\"}:\n",
    "            return getting_info_excel_csv_files(path)\n",
    "        elif path.suffix.lower() == \".docx\":\n",
    "            return get_info_from_docx(path)\n",
    "\n",
    "        mime, _ = mimetypes.guess_type(str(path))\n",
    "        if mime and mime.startswith(\"text\"):\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")[:max_chars]\n",
    "\n",
    "        if mime == \"application/pdf\":\n",
    "            with fitz.open(path) as doc:\n",
    "                return doc[0].get_text()[:max_chars]\n",
    "\n",
    "        return f\"[{mime or 'unknown'} file]\"\n",
    "\n",
    "    except Exception as exc:\n",
    "        return f\"[Error reading file: {exc}]\"\n",
    "\n",
    "\n",
    "\n",
    "def build_full_nested_directory_structure(root_dir: Path):\n",
    "    \"\"\"\n",
    "    Builds a single nested dictionary representing the entire directory structure,\n",
    "    including files and their attributes.\n",
    "\n",
    "    Args:\n",
    "        root_dir (Path): The absolute path to the root directory to scan.\n",
    "\n",
    "    Returns:\n",
    "        dict: A single nested dictionary representing the directory tree,\n",
    "              with file attributes at the leaf nodes.\n",
    "    \"\"\"\n",
    "    # Initialize the main dictionary with the root directory name as the top key\n",
    "    # Handle cases where root_dir might be a drive letter\n",
    "    root_name = root_dir.name if root_dir.name else str(root_dir)\n",
    "    full_structure = {root_name: {}}\n",
    "\n",
    "    # Iterate through all items (files and directories) within the root_dir\n",
    "    # sorted by path length to ensure parent directories are added before children\n",
    "    all_items = sorted(list_files_and_dirs(root_dir), key=lambda x: len(x[0].parts))\n",
    "\n",
    "    for item_path, is_directory in all_items:\n",
    "        try:\n",
    "            # Get the path relative to the root_dir\n",
    "            relative_path_str = os.path.relpath(str(item_path), str(root_dir))\n",
    "            # Split the relative path into components (keys)\n",
    "            # Filter out empty strings which can result from split on leading/trailing slashes\n",
    "            path_components = [part for part in Path(relative_path_str).parts if part]\n",
    "\n",
    "            if not path_components: # This can happen if item_path is the root_dir itself, which we already handled\n",
    "                continue\n",
    "\n",
    "            current_level = full_structure[root_name]\n",
    "\n",
    "            # Traverse or create nested dictionaries for directory parts\n",
    "            for i, component in enumerate(path_components):\n",
    "                if i == len(path_components) - 1: # This is the last component of the path\n",
    "                    if is_directory:\n",
    "                        # If it's a directory, ensure it's a dictionary\n",
    "                        if component not in current_level:\n",
    "                            current_level[component] = {}\n",
    "                        # If it exists and is a file, this would be an error, but our logic prevents it\n",
    "                    else:\n",
    "                        # If it's a file, assign its attributes\n",
    "                        preview = get_file_preview(item_path)\n",
    "                        current_level[component] = {\n",
    "                            \"filename\": item_path.name,\n",
    "                            \"filepath\": str(item_path),\n",
    "                            \"filetype\": item_path.suffix.lstrip(\".\").lower(),\n",
    "                            \"mime_type\": mimetypes.guess_type(str(item_path))[0] or \"\",\n",
    "                            \"preview\": preview,\n",
    "                            \"summary\": preview[:100],\n",
    "                            \"is_directory\": False # Explicitly state it's a file\n",
    "                        }\n",
    "                else:\n",
    "                    # If it's a directory part, ensure it's a dictionary for nesting\n",
    "                    if component not in current_level:\n",
    "                        current_level[component] = {}\n",
    "                    current_level = current_level[component] # Move deeper\n",
    "\n",
    "        except ValueError as e:\n",
    "            # This can happen if item_path is not truly a subpath of root_dir (e.g., different drive)\n",
    "            print(f\"Warning: Could not process {item_path} relative to {root_dir}. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred processing {item_path}: {e}\")\n",
    "\n",
    "    return full_structure\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"--- Scanning directory: {DOCUMENTS_DIR} ---\")\n",
    "    nested_directory_data = build_full_nested_directory_structure(DOCUMENTS_DIR)\n",
    "\n",
    "    print(\"\\n--- Generated Nested Directory Structure (JSON Output) ---\")\n",
    "    # Use json.dumps for pretty printing the dictionary\n",
    "    output_json= DOCUMENTS_DIR / \"full_data_file_info.json\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(nested_directory_data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Output saved to: {output_json}\")\n",
    "    #print(json.dumps(nested_directory_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7007f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added GPT metadata to all leaf records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "#Load the OpenAI API key from the .env file\n",
    "#Make sure to set the path to your .env file\n",
    "dotenv_path = r\"C:\\Users\\gabel\\Documents\\File Organizer AI\\.env\"\n",
    "if not os.path.isfile(dotenv_path):\n",
    "    raise FileNotFoundError(f\"Missing .env at {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)                     # <- pass explicit path\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY in your .env file\")\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "def gpt_summarize_and_tag(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ask GPT-4 to return JSON with keys 'summary', 'suggested_title', 'suggested_tags'.\n",
    "    The model response is parsed and returned as a Python dict.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are an assistant that summarizes files and suggests filenames & tags. \"\n",
    "        \"Use the preview and summary text to create a concise summary, if this is not available, use the title to infer a summary.\"\n",
    "        \"Always reply **only** with valid JSON having keys \"\n",
    "        \"'summary' (2-3 sentences), \"\n",
    "        \"'suggested_title' (concise filename, no extension, underscores OK), \"\n",
    "        \"'suggested_tags' (list of 3-5 short category strings).\"\n",
    "    )\n",
    "    user = f\"Here is the file content or preview:\\n{text}\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"system\", \"content\": system},\n",
    "                  {\"role\": \"user\", \"content\": user}],\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(resp.choices[0].message.content.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: wrap whole text as 'summary' if parsing fails\n",
    "        return {\n",
    "            \"summary\": text[:200],  # fallback to first 200 chars\n",
    "            \"suggested_title\": \"\",\n",
    "            \"suggested_tags\": [],\n",
    "        }\n",
    "    \n",
    "#Recurse through an arbitrary nested dict\n",
    "def enrich_leaves_with_gpt(node, *, preview_key=\"preview\"):\n",
    "    \"\"\"\n",
    "    Recursively traverse a (possibly) nested dictionary.\n",
    "\n",
    "    • If a sub-dict contains `preview_key`, we treat it as a file record,\n",
    "      call gpt_summarize_and_tag(preview_text) and merge the result.\n",
    "    • Otherwise, keep walking.\n",
    "\n",
    "    The function mutates the original structure in-place and\n",
    "    returns nothing.\n",
    "    \"\"\"\n",
    "    if isinstance(node, dict):\n",
    "        # Leaf condition: this dict already has a preview\n",
    "        if preview_key in node:\n",
    "            # --- call GPT only if we haven't done so yet --------------\n",
    "            if not {\"summary\", \"suggested_title\", \"suggested_tags\"}.issubset(node):\n",
    "                ai = gpt_summarize_and_tag(node[preview_key])\n",
    "                node.update(ai)          # merge summary/title/tags in-place\n",
    "        else:\n",
    "            # Not a leaf → recurse into all values that are dicts\n",
    "            for value in node.values():\n",
    "                enrich_leaves_with_gpt(value, preview_key=preview_key)\n",
    "\n",
    "\n",
    "#Loads Json file with all the data\n",
    "with open(DOCUMENTS_DIR / \"full_data_file_info.json\", encoding=\"utf-8\") as f:\n",
    "    combined = json.load(f)\n",
    "\n",
    "#Adds summary / title / tags to each leaf record\n",
    "enrich_leaves_with_gpt(combined)     \n",
    "\n",
    "#Write the updated data to a new file\n",
    "with open(DOCUMENTS_DIR / \"full_data_with_ai.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined, f, indent=2)\n",
    "print(\"Added GPT metadata to all leaf records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fcb7854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups:\n",
      "- Data Analysis: [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18]\n",
      "- Error Handling: [3, 19, 20, 21, 22, 23]\n",
      "- Documentation: [17, 24, 25, 26, 27, 28, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# 1) Load the nested dictionary as necessary\n",
    "with open(DOCUMENTS_DIR / \"full_data_with_ai.json\", encoding=\"utf-8\") as f:\n",
    "    nested = json.load(f)\n",
    "\n",
    "# Flatten to a list of file records to be more easily accessible\n",
    "def leaf_records(node):\n",
    "    \"\"\"Yield each dict that has a 'filename' key (leaf record).\"\"\"\n",
    "    if isinstance(node, dict):\n",
    "        if \"filename\" in node:         # <-- leaf condition\n",
    "            yield node\n",
    "        else:\n",
    "            for v in node.values():\n",
    "                yield from leaf_records(v)\n",
    "\n",
    "files_info = list(leaf_records(nested))   #Now converted to a list, good for processing and complex queries\n",
    "\n",
    "\n",
    "def suggest_file_groups(file_records):\n",
    "    \"\"\"Ask GPT to propose folder names + assignments for all records.\"\"\"\n",
    "    file_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {rec['filename']} — {rec['summary']} \"\n",
    "        f\"[Tags: {', '.join(rec.get('suggested_tags', []))}]\"\n",
    "        for i, rec in enumerate(file_records)\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"Below are files with short summaries and tags.\\n\"\n",
    "        \"Suggest 3-5 folder names that logically group them. \"\n",
    "        \"Then map each file number to a folder. \"\n",
    "        \"Respond ONLY in this format:\\n\\n\"\n",
    "        \"Groups:\\n\"\n",
    "        \"- <FolderName1>: [file#, file#, ...]\\n\"\n",
    "        \"- <FolderName2>: [...]\\n\\n\"\n",
    "        f\"{file_descriptions}\"\n",
    "    )\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",           # or gpt-4o\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "#Call the function to get the suggested folder groupings\n",
    "suggested_folders = suggest_file_groups(files_info)\n",
    "print(suggested_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99d0f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving faers_2004_2024q1.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Data Analysis and Code\\faers_2004_2024q1.csv\n",
      "Moving FAERS_paper_code.ipynb to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Data Analysis and Code\\FAERS_paper_code.ipynb\n",
      "Moving abn_behaviour_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\abn_behaviour_disprortionality.csv\n",
      "Moving aggression_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\aggression_disprortionality.csv\n",
      "Moving agitation_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\agitation_disprortionality.csv\n",
      "Moving anxiety_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\anxiety_disprortionality.csv\n",
      "Moving compl_suicide_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\compl_suicide_disprortionality.csv\n",
      "Moving depression_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\depression_disprortionality.csv\n",
      "Moving Result section Tables - Evaluating FAERS - Copy.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\Result section Tables - Evaluating FAERS - Copy.docx\n",
      "Moving Result section Tables - Evaluating FAERS.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\Result section Tables - Evaluating FAERS.docx\n",
      "Moving suicidal_beh_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\suicidal_beh_disprortionality.csv\n",
      "Moving suicide_attempt_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\suicide_attempt_disprortionality.csv\n",
      "Moving suicide_idea_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\suicide_idea_disprortionality.csv\n",
      "Moving susp_suicide_disprortionality.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\susp_suicide_disprortionality.csv\n",
      "Moving target_drugs_sample.csv to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\target_drugs_sample.csv\n",
      "Moving Unadjusted and Adjusted ROR results.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Data Analysis and Code\\Unadjusted and Adjusted ROR results.xlsx\n",
      "Moving Montelukast_stratified_disproportionality_all_cases.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Error and Dependency Issues\\Montelukast_stratified_disproportionality_all_cases.xlsx\n",
      "Moving Montelukast_stratified_disproportionality_target_cases.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Error and Dependency Issues\\Montelukast_stratified_disproportionality_target_cases.xlsx\n",
      "Moving NAE_Disproportionality_all_cases_2004-2024.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Error and Dependency Issues\\NAE_Disproportionality_all_cases_2004-2024.xlsx\n",
      "Moving NAE_Disproportionality__Montelukast_and_comparators.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Error and Dependency Issues\\NAE_Disproportionality__Montelukast_and_comparators.xlsx\n",
      "Moving suicidal_beh_disprortionality.xlsx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Error and Dependency Issues\\suicidal_beh_disprortionality.xlsx\n",
      "Moving synopsis.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Drug Safety Analysis\\synopsis.docx\n",
      "Moving Evaluating FAERS Draft with Emmanuel additions.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\FAERS Research, Analysis and Paper\\Evaluating FAERS Draft with Emmanuel additions.docx\n",
      "Moving Indigenous Mental Health Resources.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Mental Health Resources\\Indigenous Mental Health Resources.docx\n",
      "Moving AutoRecovery save of January 11 2025, Draft.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Research and Evaluation Documents\\AutoRecovery save of January 11 2025, Draft.docx\n",
      "Moving cmda4634ProjectProposal (1).pdf to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Research and Evaluation Documents\\cmda4634ProjectProposal (1).pdf\n",
      "Moving CoverLetter.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Research and Evaluation Documents\\CoverLetter.docx\n",
      "Moving Gabriel Dell.docx to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Research and Evaluation Documents\\Gabriel Dell.docx\n",
      "Moving Gabriel_Dell_Resume.pdf to C:\\Users\\gabel\\OneDrive - Virginia Tech\\Research and Evaluation Documents\\Gabriel_Dell_Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "def parse_group_response(response_text):\n",
    "    folder_map = defaultdict(list)\n",
    "    lines = response_text.splitlines()\n",
    "    for line in lines:\n",
    "        match = re.match(r\"- (.+?): \\[(.+?)\\]\", line.strip())\n",
    "        if match:\n",
    "            folder, indices = match.groups()\n",
    "            indices = [int(i.strip()) - 1 for i in indices.split(\",\")]\n",
    "            folder_map[folder].extend(indices)\n",
    "    return folder_map\n",
    "\n",
    "folder_map = parse_group_response(suggested_folders)\n",
    "\n",
    "def move_files_to_folders(folder_map, files_info):\n",
    "    for folder, indices in folder_map.items():\n",
    "        target_dir = DOCUMENTS_DIR / folder\n",
    "        target_dir.mkdir(exist_ok=True)\n",
    "        for index in indices:\n",
    "            file_info = files_info[index]\n",
    "            if(file_info[\"is_directory\"] == 0):\n",
    "                # Move the file to the target directory\n",
    "                old_path = DOCUMENTS_DIR / file_info['filepath']\n",
    "                new_path = target_dir / file_info[\"filepath\"]\n",
    "                shutil.move(str(old_path), str(new_path))\n",
    "                print(f\"Moving {file_info['filename']} to {new_path}\")\n",
    "            else:\n",
    "                old_path = file_info[\"filepath\"]\n",
    "                new_path = target_dir / file_info[\"filename\"]\n",
    "                shutil.move(str(old_path), str(new_path))\n",
    "                print(f\"Moving {file_info['filename']} to {new_path}\")\n",
    "\n",
    "# Call the function to move files based on the folder map\n",
    "move_files_to_folders(folder_map, files_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
